{
    "componentChunkName": "component---src-templates-integration-tsx",
    "path": "/integration/apache-spark/",
    "result": {"data":{"mdx":{"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"Apache Spark\",\n  \"description\": \"OpenLineage can automatically track lineage of jobs and datasets across Spark jobs.\",\n  \"image\": \"./stack.png\",\n  \"banner\": \"./banner.svg\",\n  \"version\": \"2.4+\",\n  \"datasources\": \"JDBC, HDFS, Google Cloud Storage, Google BigQuery, Amazon S3, Azure Blob Storage, Azure Data Lake Gen2, Azure Synapse\",\n  \"github\": \"https://github.com/OpenLineage/OpenLineage/tree/main/integration/spark\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"div\", {\n    className: \"table-of-contents\"\n  }, mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"#collecting-lineage-in-spark\"\n  }, \"Collecting Lineage in Spark\")), mdx(\"li\", {\n    parentName: \"ol\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"#how-to-use-the-integration\"\n  }, \"How to Use the Integration\"), mdx(\"ol\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ol\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"#sparklistener\"\n  }, \"SparkListener\"), mdx(\"ol\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ol\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"#spark-submit\"\n  }, \"spark-submit\")), mdx(\"li\", {\n    parentName: \"ol\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"#spark-defaultsconf\"\n  }, \"spark-defaults.conf\")))), mdx(\"li\", {\n    parentName: \"ol\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"#javaagent\"\n  }, \"Javaagent\"), mdx(\"ol\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ol\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"#spark-submit-1\"\n  }, \"spark-submit\")))), mdx(\"li\", {\n    parentName: \"ol\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"#from-airflow\"\n  }, \"From Airflow\")))))), mdx(\"p\", null, \"Spark jobs typically run on clusters of machines. A single machine hosts the \\\"driver\\\" application,\\nwhich constructs a graph of jobs - e.g., reading data from a source, filtering, transforming, and\\njoining records, and writing results to some sink- and manages execution of those jobs. Spark's\\nfundamental abstraction is the Resilient Distributed Dataset (RDD), which encapsulates distributed\\nreads and modifications of records. While RDDs can be used directly, it is far more common to work\\nwith Spark Datasets or Dataframes, which is an API that adds explicit schemas for better performance\\nand the ability to interact with datasets using SQL. The Dataframe's declarative API enables Spark\\nto optimize jobs by analyzing and manipulating an abstract query plan prior to execution.\"), mdx(\"h2\", {\n    \"id\": \"collecting-lineage-in-spark\"\n  }, \"Collecting Lineage in Spark\"), mdx(\"p\", null, \"Collecting lineage requires hooking into Spark's \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"ListenerBus\"), \" in the driver application and\\ncollecting and analyzing execution events as they happen. Both raw RDD and Dataframe jobs post events\\nto the listener bus during execution. These events expose the structure of the job, including the\\noptimized query plan, allowing the Spark integration to analyze the job for datasets consumed and\\nproduced, including attributes about the storage, such as location in GCS or S3, table names in a\\nrelational database or warehouse, such as Redshift or Bigquery, and schemas. In addition to dataset\\nand job lineage, Spark SQL jobs also report logical plans, which can be compared across job runs to\\ntrack important changes in query plans, which may affect the correctness or speed of a job.\"), mdx(\"p\", null, \"A single Spark application may execute multiple jobs. The Spark OpenLineage integration maps one\\nSpark job to a single OpenLineage Job. The application will be assigned a Run id at startup and each\\njob that executes will report the application's Run id as its parent job run. Thus, an application\\nthat reads one or more source datasets, writes an intermediate dataset, then transforms that\\nintermediate dataset and writes a final output dataset will report three jobs- the parent application\\njob, the initial job that reads the sources and creates the intermediate dataset, and the final job\\nthat consumes the intermediate dataset and produces the final output. As an image:\\n\", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"1200px\",\n      \"margin\": \"3rem\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/92dd1101586659a5307c25291e4ac60d/c9c3a/spark-job-creation.dot.png\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"15.666666666666668%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAADCAIAAAAcOLh5AAAACXBIWXMAAAsTAAALEwEAmpwYAAAAoUlEQVQI1z3LXQuCMBiGYf//L4gOotOgw4iIIgv6wAzUGovMYFNjBXOYQux9lQWLuo7ug+dx0iwPI8J4BlbTNL9AY7VtCwBo/TfGGABwvEPU7Q+Xa18pJaWklCZJgohSFnN3O5q4hF7fFiGEMYaI4vEcT1eeHzq7fdDpDWaLTV3XVVVxzvM811qX5YvQODhSnt6/Z8aYEAK0LgoVns6X+PYBy9ih/p8ULlMAAAAASUVORK5CYII=')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"image\",\n    \"title\": \"image\",\n    \"src\": \"/static/92dd1101586659a5307c25291e4ac60d/c1b63/spark-job-creation.dot.png\",\n    \"srcSet\": [\"/static/92dd1101586659a5307c25291e4ac60d/5a46d/spark-job-creation.dot.png 300w\", \"/static/92dd1101586659a5307c25291e4ac60d/0a47e/spark-job-creation.dot.png 600w\", \"/static/92dd1101586659a5307c25291e4ac60d/c1b63/spark-job-creation.dot.png 1200w\", \"/static/92dd1101586659a5307c25291e4ac60d/c9c3a/spark-job-creation.dot.png 1289w\"],\n    \"sizes\": \"(max-width: 1200px) 100vw, 1200px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n  \"), \"\\n    \")), mdx(\"h2\", {\n    \"id\": \"how-to-use-the-integration\"\n  }, \"How to Use the Integration\"), mdx(\"p\", null, \"Adding OpenLineage metadata collection to existing Spark jobs was designed to be straightforward\\nand unobtrusive to the application. The Spark integration works as either a \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"javaagent\"), \" or as a\\n\", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"SparkListener\"), \".\"), mdx(\"h3\", {\n    \"id\": \"sparklistener\"\n  }, \"SparkListener\"), mdx(\"p\", null, \"The \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"SparkListener\"), \" approach is very simple and covers most cases. The listener simply analyzes\\nevents, as they are posted by the SparkContext, and extracts job and dataset metadata that are\\nexposed by the RDD and Dataframe dependency graphs. Most data sources, such as filesystem sources\\n(including S3 and GCS), JDBC backends, and warehouses such as Redshift and Bigquery can be analyzed\\nand reported in this way.\"), mdx(\"h4\", {\n    \"id\": \"spark-submit\"\n  }, \"spark-submit\"), mdx(\"p\", null, \"The listener can be enabled by adding the following configuration to a \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"spark-submit\"), \" command:\"), mdx(\"deckgo-highlight-code\", {\n    \"language\": \"bash\",\n    \"terminal\": \"carbon\"\n  }, \"\\n          \", mdx(\"code\", {\n    parentName: \"deckgo-highlight-code\",\n    \"slot\": \"code\"\n  }, \"spark-submit --conf \\\"spark.extraListeners=io.openlineage.spark.agent.OpenLineageSparkListener\\\" \\\\\\n    --packages \\\"io.openlineage:openlineage-spark:0.2.+\\\" \\\\\\n    --conf \\\"spark.openlineage.host=http://<your_ol_endpoint>\\\" \\\\\\n    --conf \\\"spark.openlineage.namespace=my_job_namespace\\\" \\\\\\n    --class com.mycompany.MySparkApp my_application.jar\"), \"\\n        \"), mdx(\"p\", null, \"Additional configuration can be set if applicable\"), mdx(\"table\", null, mdx(\"tbody\", null, mdx(\"tr\", null, mdx(\"th\", null, \"Configuration Key\"), mdx(\"th\", null, \"Description\"), mdx(\"th\", null, \"Default\")), mdx(\"tr\", null, mdx(\"td\", null, \"spark.openlineage.parentJobName\"), mdx(\"td\", null, \"The job name of the parent job that triggered this Spark application\"), mdx(\"td\", null)), mdx(\"tr\", null, mdx(\"td\", null, \"spark.openlineage.parentRunId\"), mdx(\"td\", null, \"The RunId of the parent job Run that triggered this Spark application\"), mdx(\"td\", null, \"\\xA0\")), mdx(\"tr\", null, mdx(\"td\", null, \"spark.openlineage.apiKey\"), mdx(\"td\", null, \"The API Key used to authenticate with the OpenLineage server that collects events\"), mdx(\"td\", null, \"\\xA0\")), mdx(\"tr\", null, mdx(\"td\", null, \"spark.openlineage.version\"), mdx(\"td\", null, \"The API version of the OpenLineage specification\"), mdx(\"td\", null, \"1\")))), mdx(\"h4\", {\n    \"id\": \"spark-defaultsconf\"\n  }, \"spark-defaults.conf\"), mdx(\"p\", null, \"Alternatively, the same configuration parameters can be added to the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"spark-defaults.conf\"), \" file on\\ncluster creation. Add the following key/value parameters to the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"spark-defaults.conf\"), \" file:\"), mdx(\"deckgo-highlight-code\", {\n    \"terminal\": \"carbon\"\n  }, \"\\n          \", mdx(\"code\", {\n    parentName: \"deckgo-highlight-code\",\n    \"slot\": \"code\"\n  }, \"spark.jars.packages    io.openlineage:openlineage-spark:0.2.+\\nspark.extraListeners   io.openlineage.spark.agent.OpenLineageSparkListener\\nspark.openlineage.host       http://<your_ol_endpoint>\\nspark.openlineage.namespace  my_job_namespace\"), \"\\n        \"), mdx(\"p\", null, \"The optional keys defined above can also be added here. When the job is submitted, additional or\\noverriding configuration values can be supplied. E.g., the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"spark.openlineage.host\"), \" and \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"spark.openlineage.namespace\"), \"\\ncan be defined in the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"spark-defaults.conf\"), \" file and the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"spark.openlineage.parentRunId\"), \" and \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"spark.openlineage.parentJobName\"), \"\\nconfiguration can be supplied when the job is submitted by the parent job.\"), mdx(\"h3\", {\n    \"id\": \"javaagent\"\n  }, \"Javaagent\"), mdx(\"p\", null, \"The Javaagent approach is the earliest approach to adding lineage events. It was aimed to support\\ninstrumenting Spark code directly by manipulating bytecode at runtime. In the case of Dataframe or\\nRDD code that doesn't expose the underlying datasource directly, the javaagent approach will allow\\ninjecting bytecode at runtime to expose the required information. This approach requires being able\\nto add the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"openlineage-spark\"), \" jar on the driver host and adding the correct JVM startup parameters. This\\nmay not be possible, e.g., on a serverless Spark platform, such as AWS Glue.\"), mdx(\"h4\", {\n    \"id\": \"spark-submit-1\"\n  }, \"spark-submit\"), mdx(\"p\", null, \"The following configuration must be added to the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"spark-submit\"), \" command when the job is submitted:\"), mdx(\"deckgo-highlight-code\", {\n    \"language\": \"bash\",\n    \"terminal\": \"carbon\"\n  }, \"\\n          \", mdx(\"code\", {\n    parentName: \"deckgo-highlight-code\",\n    \"slot\": \"code\"\n  }, \"spark-submit --conf spark.driver.extraJavaOptions=-javaagent:<openlineage-spark-jar-location>=http://<your_ol_endpoint>/api/v1/namespaces/<your_job_namespace>/?api_key=<optional_api_key>\"), \"\\n        \"), mdx(\"p\", null, \"If a parent job run is triggering the Spark job run, the parent job's name and Run id can be included as such:\"), mdx(\"deckgo-highlight-code\", {\n    \"language\": \"bash\",\n    \"terminal\": \"carbon\"\n  }, \"\\n          \", mdx(\"code\", {\n    parentName: \"deckgo-highlight-code\",\n    \"slot\": \"code\"\n  }, \"spark-submit --conf spark.driver.extraJavaOptions=-javaagent:<openlineage-spark-jar-location>=http://<your_ol_endpoint>/api/v1/namespaces/<your_job_namespace>/jobs/<parent_job_name>/runs/<parent_run_id>?api_key=<optional_api_key>\"), \"\\n        \"), mdx(\"h3\", {\n    \"id\": \"from-airflow\"\n  }, \"From Airflow\"), mdx(\"p\", null, \"The same parameters passed to \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"spark-submit\"), \" can be supplied from Airflow and other schedulers. If\\nusing the \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"../apache-airflow/\"\n  }, \"openlineage-airflow\"), \" integration, each task in the DAG has its own Run id\\nwhich can be connected to the Spark job run via the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"spark.openlineage.parentRunId\"), \" parameter. For example,\\nhere is an example of a \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"DataProcPySparkOperator\"), \" that submits a Pyspark application on Dataproc:\"), mdx(\"deckgo-highlight-code\", {\n    \"language\": \"python\",\n    \"terminal\": \"carbon\"\n  }, \"\\n          \", mdx(\"code\", {\n    parentName: \"deckgo-highlight-code\",\n    \"slot\": \"code\"\n  }, \"t1 = DataProcPySparkOperator(\\n    task_id=job_name,\\n    gcp_conn_id='google_cloud_default',\\n    project_id='project_id',\\n    cluster_name='cluster-name',\\n    region='us-west1',\\n    main='gs://bucket/your-prog.py',\\n    job_name=job_name,\\n    dataproc_pyspark_properties={\\n      \\\"spark.extraListeners\\\": \\\"io.openlineage.spark.agent.OpenLineageSparkListener\\\",\\n      \\\"spark.jars.packages\\\": \\\"io.openlineage:openlineage-spark:0.2.+\\\",\\n      \\\"spark.openlineage.url\\\": f\\\"{openlineage_url}/api/v1/namespaces/{openlineage_namespace}/jobs/dump_orders_to_gcs/runs/{{{{task_run_id(run_id, task)}}}}?api_key={api_key}\\\"\\n    },\\n    dag=dag)\"), \"\\n        \"), mdx(\"p\", null, \"The same job can be submitted using the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"javaagent\"), \" approach:\"), mdx(\"deckgo-highlight-code\", {\n    \"language\": \"python\",\n    \"terminal\": \"carbon\"\n  }, \"\\n          \", mdx(\"code\", {\n    parentName: \"deckgo-highlight-code\",\n    \"slot\": \"code\"\n  }, \"t1 = DataProcPySparkOperator(\\n    task_id=job_name,\\n    gcp_conn_id='google_cloud_default',\\n    project_id='project_id',\\n    cluster_name='cluster-name',\\n    region='us-west1',\\n    main='gs://bucket/your-prog.py',\\n    job_name=job_name,\\n    dataproc_pyspark_properties={\\n      'spark.driver.extraJavaOptions':\\n        f\\\"-javaagent:openlineage-spark-0.2.2.jar={openlineage_url}/api/v1/namespaces/{openlineage_namespace}/jobs/dump_orders_to_gcs/runs/{{{{task_run_id(run_id, task)}}}}?api_key={api_key}\\\"\\n    files=\\\"https://repo1.maven.org/maven2/io/openlineage/openlineage-spark/0.2.2/openlineage-spark-0.2.2.jar\\\",\\n    dag=dag)\"), \"\\n        \"));\n}\n;\nMDXContent.isMDXComponent = true;","frontmatter":{"title":"Apache Spark","date":null,"description":"OpenLineage can automatically track lineage of jobs and datasets across Spark jobs.","banner":{"publicURL":"/static/abdacb305e992063eed463fbf7b58ca0/banner.svg","childImageSharp":null}}}},"pageContext":{"slug":"/integration/apache-spark/"}},
    "staticQueryHashes": ["1139857438","1946588481","2083862410","2213455283","2418326273","3067102388"]}